{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2bc468e",
   "metadata": {
    "id": "b2bc468e"
   },
   "source": [
    "# Legal Case Summarization Assistant\n",
    "## Domain-Specific Fine-Tuning with LoRA\n",
    "\n",
    "**Project Overview:**\n",
    "This notebook implements a production-quality legal case summarization system by fine-tuning a lightweight generative LLM using Parameter-Efficient Fine-Tuning (PEFT) with LoRA on the `joelniklaus/legal_case_document_summarization` dataset.\n",
    "\n",
    "**Key Components:**\n",
    "- Dataset: Legal court judgments with human-written summaries\n",
    "- Model: Gemma-2B with 4-bit quantization\n",
    "- Training: LoRA for parameter-efficient fine-tuning\n",
    "- Evaluation: ROUGE metrics comparing base vs fine-tuned models\n",
    "- Deployment: Gradio interface for interactive inference\n",
    "\n",
    "**Author:** ML Engineering Project  \n",
    "**Date:** February 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668b33e",
   "metadata": {
    "id": "4668b33e"
   },
   "source": [
    "---\n",
    "## 1. Environment Setup\n",
    "\n",
    "Install required dependencies and configure the runtime environment for efficient training on Colab's free GPU (T4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4378fea2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:27.105958Z",
     "iopub.status.busy": "2026-02-15T19:18:27.105347Z",
     "iopub.status.idle": "2026-02-15T19:18:53.619442Z",
     "shell.execute_reply": "2026-02-15T19:18:53.618716Z",
     "shell.execute_reply.started": "2026-02-15T19:18:27.105927Z"
    },
    "id": "4378fea2",
    "outputId": "ba615996-2c90-406d-9953-7dc6d5446682",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upgrading core packages to latest versions...\n",
      "Collecting transformers\n",
      "  Downloading transformers-5.1.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting packaging>=20.0 (from transformers)\n",
      "  Downloading packaging-26.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m229.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Collecting typer-slim (from transformers)\n",
      "  Downloading typer_slim-0.23.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m223.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from huggingface_hub)\n",
      "  Downloading filelock-3.24.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub)\n",
      "  Downloading fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface_hub)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting shellingham (from huggingface_hub)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typing-extensions>=4.1.0 (from huggingface_hub)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface_hub)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->huggingface_hub)\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface_hub)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.23.0->huggingface_hub)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting typer>=0.23.1 (from typer-slim->transformers)\n",
      "  Downloading typer-0.23.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting click>=8.0.0 (from typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting rich>=10.11.0 (from typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading rich-14.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting annotated-doc>=0.0.2 (from typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading pygments-2.19.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->transformers)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading transformers-5.1.0-py3-none-any.whl (10.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m167.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m385.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2026.2.0-py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m202.5/202.5 kB\u001b[0m \u001b[31m360.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m331.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m259.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m243.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m333.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading packaging-26.0-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m74.4/74.4 kB\u001b[0m \u001b[31m333.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m362.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2026.1.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m348.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m507.2/507.2 kB\u001b[0m \u001b[31m368.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m228.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.67.3-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m274.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m178.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.24.0-py3-none-any.whl (23 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.23.1-py3-none-any.whl (3.4 kB)\n",
      "Downloading typer-0.23.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m299.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m306.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m325.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m283.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m353.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading rich-14.3.2-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m310.0/310.0 kB\u001b[0m \u001b[31m360.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m87.3/87.3 kB\u001b[0m \u001b[31m265.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pygments-2.19.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m205.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, shellingham, safetensors, regex, pyyaml, pygments, packaging, numpy, mdurl, idna, hf-xet, h11, fsspec, filelock, click, certifi, annotated-doc, markdown-it-py, httpcore, anyio, rich, httpx, typer, typer-slim, huggingface_hub, tokenizers, transformers\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.15.0\n",
      "    Uninstalling typing_extensions-4.15.0:\n",
      "      Successfully uninstalled typing_extensions-4.15.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.67.3\n",
      "    Uninstalling tqdm-4.67.3:\n",
      "      Successfully uninstalled tqdm-4.67.3\n",
      "  Attempting uninstall: shellingham\n",
      "    Found existing installation: shellingham 1.5.4\n",
      "    Uninstalling shellingham-1.5.4:\n",
      "      Successfully uninstalled shellingham-1.5.4\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.7.0\n",
      "    Uninstalling safetensors-0.7.0:\n",
      "      Successfully uninstalled safetensors-0.7.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2026.1.15\n",
      "    Uninstalling regex-2026.1.15:\n",
      "      Successfully uninstalled regex-2026.1.15\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 6.0.3\n",
      "    Uninstalling PyYAML-6.0.3:\n",
      "      Successfully uninstalled PyYAML-6.0.3\n",
      "  Attempting uninstall: pygments\n",
      "    Found existing installation: Pygments 2.19.2\n",
      "    Uninstalling Pygments-2.19.2:\n",
      "      Successfully uninstalled Pygments-2.19.2\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 26.0\n",
      "    Uninstalling packaging-26.0:\n",
      "      Successfully uninstalled packaging-26.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.4.2\n",
      "    Uninstalling numpy-2.4.2:\n",
      "      Successfully uninstalled numpy-2.4.2\n",
      "  Attempting uninstall: mdurl\n",
      "    Found existing installation: mdurl 0.1.2\n",
      "    Uninstalling mdurl-0.1.2:\n",
      "      Successfully uninstalled mdurl-0.1.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.11\n",
      "    Uninstalling idna-3.11:\n",
      "      Successfully uninstalled idna-3.11\n",
      "  Attempting uninstall: hf-xet\n",
      "    Found existing installation: hf-xet 1.2.0\n",
      "    Uninstalling hf-xet-1.2.0:\n",
      "      Successfully uninstalled hf-xet-1.2.0\n",
      "  Attempting uninstall: h11\n",
      "    Found existing installation: h11 0.16.0\n",
      "    Uninstalling h11-0.16.0:\n",
      "      Successfully uninstalled h11-0.16.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.10.0\n",
      "    Uninstalling fsspec-2025.10.0:\n",
      "      Successfully uninstalled fsspec-2025.10.0\n",
      "  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.24.0\n",
      "    Uninstalling filelock-3.24.0:\n",
      "      Successfully uninstalled filelock-3.24.0\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.3.1\n",
      "    Uninstalling click-8.3.1:\n",
      "      Successfully uninstalled click-8.3.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2026.1.4\n",
      "    Uninstalling certifi-2026.1.4:\n",
      "      Successfully uninstalled certifi-2026.1.4\n",
      "  Attempting uninstall: annotated-doc\n",
      "    Found existing installation: annotated-doc 0.0.4\n",
      "    Uninstalling annotated-doc-0.0.4:\n",
      "      Successfully uninstalled annotated-doc-0.0.4\n",
      "  Attempting uninstall: markdown-it-py\n",
      "    Found existing installation: markdown-it-py 4.0.0\n",
      "    Uninstalling markdown-it-py-4.0.0:\n",
      "      Successfully uninstalled markdown-it-py-4.0.0\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 1.0.9\n",
      "    Uninstalling httpcore-1.0.9:\n",
      "      Successfully uninstalled httpcore-1.0.9\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.12.1\n",
      "    Uninstalling anyio-4.12.1:\n",
      "      Successfully uninstalled anyio-4.12.1\n",
      "  Attempting uninstall: rich\n",
      "    Found existing installation: rich 14.3.2\n",
      "    Uninstalling rich-14.3.2:\n",
      "      Successfully uninstalled rich-14.3.2\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.28.1\n",
      "    Uninstalling httpx-0.28.1:\n",
      "      Successfully uninstalled httpx-0.28.1\n",
      "  Attempting uninstall: typer\n",
      "    Found existing installation: typer 0.23.1\n",
      "    Uninstalling typer-0.23.1:\n",
      "      Successfully uninstalled typer-0.23.1\n",
      "  Attempting uninstall: typer-slim\n",
      "    Found existing installation: typer-slim 0.23.1\n",
      "    Uninstalling typer-slim-0.23.1:\n",
      "      Successfully uninstalled typer-slim-0.23.1\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface_hub 1.4.1\n",
      "    Uninstalling huggingface_hub-1.4.1:\n",
      "      Successfully uninstalled huggingface_hub-1.4.1\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.2\n",
      "    Uninstalling tokenizers-0.22.2:\n",
      "      Successfully uninstalled tokenizers-0.22.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 5.1.0\n",
      "    Uninstalling transformers-5.1.0:\n",
      "      Successfully uninstalled transformers-5.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "datasets 4.5.0 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.2.0 which is incompatible.\n",
      "ydata-profiling 4.18.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.47.0 which is incompatible.\n",
      "google-colab 1.0.0 requires jupyter-server==2.14.0, but you have jupyter-server 2.12.5 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.1.0 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\n",
      "cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\n",
      "langchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.3.2 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.2 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\n",
      "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\n",
      "fastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\n",
      "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
      "pylibcudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-doc-0.0.4 anyio-4.12.1 certifi-2026.1.4 click-8.3.1 filelock-3.24.0 fsspec-2026.2.0 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-1.4.1 idna-3.11 markdown-it-py-4.0.0 mdurl-0.1.2 numpy-2.4.2 packaging-26.0 pygments-2.19.2 pyyaml-6.0.3 regex-2026.1.15 rich-14.3.2 safetensors-0.7.0 shellingham-1.5.4 tokenizers-0.22.2 tqdm-4.67.3 transformers-5.1.0 typer-0.23.1 typer-slim-0.23.1 typing-extensions-4.15.0\n",
      "\n",
      "Installing other required packages...\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.26.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "sentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.1.0 which is incompatible.\n",
      "cudf-cu12 25.6.0 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.26.0 requires rich<14,>=12.4.4, but you have rich 14.3.2 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "============================================================\n",
      "\u2713 All packages installed successfully\n",
      "============================================================\n",
      "\u26a0\ufe0f  CRITICAL: RESTART KERNEL NOW!\n",
      "   Kaggle: Session \u2192 Restart Session (or Ctrl+M+.)\n",
      "   Then re-run cells 4-5 to verify installation\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "# CRITICAL: Force upgrade all packages to ensure compatibility\n",
    "print(\"Upgrading core packages to latest versions...\")\n",
    "\n",
    "# Upgrade transformers and huggingface_hub together for compatibility\n",
    "!pip install --upgrade --force-reinstall --no-cache-dir transformers huggingface_hub\n",
    "\n",
    "print(\"\\nInstalling other required packages...\")\n",
    "!pip install --upgrade -q datasets \\\n",
    "                peft \\\n",
    "                accelerate \\\n",
    "                bitsandbytes \\\n",
    "                evaluate \\\n",
    "                rouge_score \\\n",
    "                gradio \\\n",
    "                sentencepiece \\\n",
    "                protobuf\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\u2713 All packages installed successfully\")\n",
    "print(\"=\"*60)\n",
    "print(\"\u26a0\ufe0f  CRITICAL: RESTART KERNEL NOW!\")\n",
    "print(\"   Kaggle: Session \u2192 Restart Session (or Ctrl+M+.)\")\n",
    "print(\"   Then re-run cells 4-5 to verify installation\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8139f25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:18:53.621220Z",
     "iopub.status.busy": "2026-02-15T19:18:53.620971Z",
     "iopub.status.idle": "2026-02-15T19:19:06.626594Z",
     "shell.execute_reply": "2026-02-15T19:19:06.625935Z",
     "shell.execute_reply.started": "2026-02-15T19:18:53.621192Z"
    },
    "id": "c8139f25",
    "outputId": "f896a9ed-f885-463b-e7a2-99e4d3f4202a",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    PeftModel\n",
    ")\n",
    "import evaluate\n",
    "import gradio as gr\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c4cbed0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:06.627762Z",
     "iopub.status.busy": "2026-02-15T19:19:06.627446Z",
     "iopub.status.idle": "2026-02-15T19:19:06.634776Z",
     "shell.execute_reply": "2026-02-15T19:19:06.633994Z",
     "shell.execute_reply.started": "2026-02-15T19:19:06.627734Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VERSION CHECK\n",
      "============================================================\n",
      "Transformers version: 5.1.0\n",
      "\n",
      "\u2713 Transformers version is compatible with Gemma\n",
      "  (Requires 4.38.0+, you have 5.1.0)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify transformers version (must be 4.38.0+ for Gemma support)\n",
    "import transformers\n",
    "print(\"=\"*60)\n",
    "print(\"VERSION CHECK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print()\n",
    "\n",
    "# Parse version\n",
    "version_parts = transformers.__version__.split('.')\n",
    "major = int(version_parts[0])\n",
    "minor = int(version_parts[1]) if len(version_parts) > 1 else 0\n",
    "\n",
    "if major > 4 or (major == 4 and minor >= 38):\n",
    "    print(\"\u2713 Transformers version is compatible with Gemma\")\n",
    "    print(\"  (Requires 4.38.0+, you have {})\".format(transformers.__version__))\n",
    "else:\n",
    "    print(\"\u274c ERROR: Transformers version too old!\")\n",
    "    print(f\"  Current: {transformers.__version__}\")\n",
    "    print(\"  Required: 4.38.0+\")\n",
    "    print()\n",
    "    print(\"FIX:\")\n",
    "    print(\"  1. Go back to Cell 3\")\n",
    "    print(\"  2. Re-run the installation cell\")\n",
    "    print(\"  3. RESTART KERNEL (Session \u2192 Restart Session)\")\n",
    "    print(\"  4. Re-run this cell\")\n",
    "    raise RuntimeError(\"Transformers version incompatible with Gemma\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62dd84f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:06.637091Z",
     "iopub.status.busy": "2026-02-15T19:19:06.636497Z",
     "iopub.status.idle": "2026-02-15T19:19:06.847488Z",
     "shell.execute_reply": "2026-02-15T19:19:06.846724Z",
     "shell.execute_reply.started": "2026-02-15T19:19:06.637069Z"
    },
    "id": "62dd84f5",
    "outputId": "9bc974ab-e9f5-41b0-e38d-9ef4b1847800",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HARDWARE CONFIGURATION\n",
      "============================================================\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "CUDA Version: 12.6\n",
      "Total GPU Memory: 17.06 GB\n",
      "Available GPU Memory: 16.79 GB\n",
      "PyTorch Version: 2.8.0+cu126\n",
      "Device: cuda\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Detect and print GPU information\n",
    "print(\"=\"*60)\n",
    "print(\"HARDWARE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Available GPU Memory: {torch.cuda.mem_get_info()[0] / 1e9:.2f} GB\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  WARNING: No GPU detected. Training will be extremely slow.\")\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85b7d87e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:06.848596Z",
     "iopub.status.busy": "2026-02-15T19:19:06.848368Z",
     "iopub.status.idle": "2026-02-15T19:19:06.860071Z",
     "shell.execute_reply": "2026-02-15T19:19:06.859397Z",
     "shell.execute_reply.started": "2026-02-15T19:19:06.848575Z"
    },
    "id": "85b7d87e",
    "outputId": "c3ac19b4-6bab-4324-b2b3-00cd6cbb8ad0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Random seed set to 42 for reproducibility\n"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"Set seed for reproducibility across all random number generators.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "print(f\"\u2713 Random seed set to {SEED} for reproducibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6893e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:06.862112Z",
     "iopub.status.busy": "2026-02-15T19:19:06.861617Z",
     "iopub.status.idle": "2026-02-15T19:19:06.924741Z",
     "shell.execute_reply": "2026-02-15T19:19:06.924193Z",
     "shell.execute_reply.started": "2026-02-15T19:19:06.862091Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING INTERNET CONNECTION\n",
      "============================================================\n",
      "\u2713 Internet connection: ACTIVE\n",
      "\u2713 Can access Hugging Face\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check internet connectivity (required for Kaggle)\n",
    "import urllib.request\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CHECKING INTERNET CONNECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    urllib.request.urlopen('https://huggingface.co', timeout=5)\n",
    "    print(\"\u2713 Internet connection: ACTIVE\")\n",
    "    print(\"\u2713 Can access Hugging Face\")\n",
    "except:\n",
    "    print(\"\u274c NO INTERNET CONNECTION!\")\n",
    "    print()\n",
    "    raise ConnectionError(\"Internet not enabled in Kaggle notebook\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff69168",
   "metadata": {
    "id": "eff69168"
   },
   "source": [
    "---\n",
    "## 2. Load Dataset\n",
    "\n",
    "Load the legal case summarization dataset from Hugging Face and perform exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0778bb55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:06.926124Z",
     "iopub.status.busy": "2026-02-15T19:19:06.925825Z",
     "iopub.status.idle": "2026-02-15T19:19:16.454681Z",
     "shell.execute_reply": "2026-02-15T19:19:16.453955Z",
     "shell.execute_reply.started": "2026-02-15T19:19:06.926095Z"
    },
    "id": "0778bb55",
    "outputId": "8e6e342d-2f65-473e-fef6-5fabaa781457",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from Hugging Face...\n",
      "Dataset: joelniklaus/legal_case_document_summarization\n",
      "Config: en (English)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f973b9b24c2f408ba74c889ccc14bc7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c4b871c019411299a23a1a3c611db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.jsonl.xz:   0%|          | 0.00/50.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc57140048a84e638093c477de505403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.jsonl.xz:   0%|          | 0.00/2.30M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3521892b112045b3befc36f2e55ac664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7773 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4458e86dc79046dfaa11c43fa0dbf6e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Dataset loaded successfully\n",
      "\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['judgement', 'dataset_name', 'summary'],\n",
      "        num_rows: 7773\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['judgement', 'dataset_name', 'summary'],\n",
      "        num_rows: 200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from Hugging Face\n",
    "print(\"Loading dataset from Hugging Face...\")\n",
    "print(\"Dataset: joelniklaus/legal_case_document_summarization\")\n",
    "print(\"Config: en (English)\")\n",
    "print()\n",
    "\n",
    "# Load the dataset - use \"en\" for English, not \"default\"\n",
    "dataset = load_dataset(\"joelniklaus/legal_case_document_summarization\", \"default\")\n",
    "\n",
    "print(\"\u2713 Dataset loaded successfully\")\n",
    "print()\n",
    "print(\"Dataset structure:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb9d9d4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:16.455923Z",
     "iopub.status.busy": "2026-02-15T19:19:16.455626Z",
     "iopub.status.idle": "2026-02-15T19:19:16.464243Z",
     "shell.execute_reply": "2026-02-15T19:19:16.463657Z",
     "shell.execute_reply.started": "2026-02-15T19:19:16.455900Z"
    },
    "id": "cfb9d9d4",
    "outputId": "af611146-690a-47fa-bd97-75761fcb3f63",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET INSPECTION\n",
      "============================================================\n",
      "\n",
      "Dataset splits: ['train', 'test']\n",
      "Features: {'judgement': Value('string'), 'dataset_name': Value('string'), 'summary': Value('string')}\n",
      "\n",
      "Training samples: 7773\n",
      "Test samples: 200\n",
      "\n",
      "============================================================\n",
      "SAMPLE EXAMPLE\n",
      "============================================================\n",
      "\n",
      "Judgment (first 500 chars):\n",
      "Appeal No. LXVI of 1949.\n",
      "Appeal from the High Court of judicature, Bombay, in a reference under section 66 of the Indian Income tax Act, 1022.\n",
      "K.M. Munshi (N. P. Nathvani, with him), for the appel lant. ' M.C. Setalvad, Attorney General for India (H. J. Umrigar, with him), for the respondent. 1950.\n",
      "May 26.\n",
      "The judgment of the Court was delivered by MEHR CHAND MAHAJAN J.\n",
      "This is an appeal against a judgment of the High Court of Judicature at Bombay in an income tax matter and it raises the questi...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Summary:\n",
      "The charge created in respect of municipal property tax by section 212 of the City of Bombay Municipal Act, 1888, is an \"annual charge not being a capital charge\" within the mean ing of section 9 (1) (iv) of the Indian Income tax Act, 199.2, and the amount of such charge should therefore be deducted in computing the income from such property for the purposes of section 9 of the Indian Income tax Act.\n",
      "The charge in respect of urban immoveable property tax created by the Bombay Finance Act, 1939 is similar in character and the amount of such charge should also be deducted.\n",
      "The expression \"capital charge\" in s.9(1) (iv) means a charge created for a capital sum,that is to say, a charge created to. ' secure the discharge of a liability of a capi tal nature; and an \"annual charge\" means a charge to secure an annual liabili ty. 554\n",
      "\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Inspect dataset structure and print example\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET INSPECTION\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(f\"Dataset splits: {list(dataset.keys())}\")\n",
    "print(f\"Features: {dataset['train'].features}\")\n",
    "print()\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "# Removed the problematic line as no 'validation' split exists\n",
    "print(f\"Test samples: {len(dataset['test'])}\")\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"SAMPLE EXAMPLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\nJudgment (first 500 chars):\\n{sample['judgement'][:500]}...\")\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"\\nSummary:\\n{sample['summary']}\")\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a09ff811",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:16.465422Z",
     "iopub.status.busy": "2026-02-15T19:19:16.465124Z",
     "iopub.status.idle": "2026-02-15T19:19:16.591013Z",
     "shell.execute_reply": "2026-02-15T19:19:16.590414Z",
     "shell.execute_reply.started": "2026-02-15T19:19:16.465394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "HUGGING FACE AUTHENTICATION\n",
      "============================================================\n",
      "Gemma is a gated model - you need to authenticate\n",
      "\n",
      "\u2713 Authentication successful!\n",
      "\n",
      "IMPORTANT: Make sure you've accepted the Gemma license:\n",
      "Visit: https://huggingface.co/google/gemma-2b\n",
      "Click 'Agree and access repository'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Login to Hugging Face to access gated models (like Gemma)\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HUGGING FACE AUTHENTICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Gemma is a gated model - you need to authenticate\")\n",
    "print()\n",
    "\n",
    "# For Kaggle: Use your token directly\n",
    "HF_TOKEN = \"YOUR_HUGGINGFACE_TOKEN_HERE\"\n",
    "\n",
    "try:\n",
    "    # Login with token\n",
    "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
    "    print(\"\u2713 Authentication successful!\")\n",
    "    print()\n",
    "    print(\"IMPORTANT: Make sure you've accepted the Gemma license:\")\n",
    "    print(\"Visit: https://huggingface.co/google/gemma-2b\")\n",
    "    print(\"Click 'Agree and access repository'\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Authentication failed: {e}\")\n",
    "    print()\n",
    "    print(\"To fix:\")\n",
    "    print(\"1. Get your token from: https://huggingface.co/settings/tokens\")\n",
    "    print(\"2. Replace HF_TOKEN value above with your token\")\n",
    "    print(\"3. Accept Gemma license: https://huggingface.co/google/gemma-2b\")\n",
    "    raise\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f0b7ca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 811,
     "referenced_widgets": [
      "8b9d667e9cce4a4fb2ceb89ff6d0c8e9",
      "d4709ba419af4daf81a9ab6460529c01",
      "1debd16c33eb4a1d95c4b998a207a4c3",
      "6d7c426f88a245c0980a6fa5741e567a",
      "75b45c1b4f3948378a5d859218e328f7",
      "241c5cdd0ed147b087ac1953aa4cf0bd",
      "ed73633671b8498d8d1f54bed48351b0",
      "e8479e25301540e5a5cc992fffc30ba3",
      "7c65a1951a0e4056a7739c6044f2cc62",
      "30d94bc074494b7fbff91a429f569a93",
      "d2ba4b82d328450c8b347a7e803c994c",
      "4d96aefa513c4337b12f420e0d0628bd",
      "46e986c818304a03a8398ab1aec4ab33",
      "2d5810aec0a542bea28e9969067622a1",
      "e6916c675b694d05989e51af2a1f5521",
      "ee1f8bf4d7f0470c9a4857ced38af1a7",
      "0f86fbb34bf74827a74479df986d84a8",
      "517d4be49d9247fc9ef7e1d62113d938",
      "7abc9e13d6c44b8f9878679abc6da763",
      "3c151dbc6db44a059dc97221a87e9612"
     ]
    },
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:16.593813Z",
     "iopub.status.busy": "2026-02-15T19:19:16.593385Z",
     "iopub.status.idle": "2026-02-15T19:19:36.421995Z",
     "shell.execute_reply": "2026-02-15T19:19:36.421242Z",
     "shell.execute_reply.started": "2026-02-15T19:19:16.593790Z"
    },
    "id": "4f0b7ca1",
    "outputId": "739ce96d-4dcf-440b-87c4-4d13eaa24fa3",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Gemma tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c04891cd124440ea0810ddb33d91fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e236fc2980a643068e303abd3796ce7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/33.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed2500eb50974c3099998bd6ee6a1a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d3b78ccd844e1b9a6fca6083d2bc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing token lengths (sampling 1000 examples)...\n",
      "\n",
      "============================================================\n",
      "TOKEN LENGTH STATISTICS\n",
      "============================================================\n",
      "\n",
      "Judgment tokens:\n",
      "  Mean: 7065\n",
      "  Median: 4475\n",
      "  Min: 319\n",
      "  Max: 101787\n",
      "  95th percentile: 21519\n",
      "\n",
      "Summary tokens:\n",
      "  Mean: 1125\n",
      "  Median: 898\n",
      "  Min: 57\n",
      "  Max: 11965\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze token lengths in the dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer for analysis (using Gemma)\n",
    "print(\"Loading Gemma tokenizer...\")\n",
    "analysis_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "\n",
    "def get_token_stats(dataset_split, num_samples=1000):\n",
    "    \"\"\"Calculate token length statistics for judgments.\"\"\"\n",
    "    judgment_lengths = []\n",
    "    summary_lengths = []\n",
    "\n",
    "    sample_size = min(num_samples, len(dataset_split))\n",
    "    indices = random.sample(range(len(dataset_split)), sample_size)\n",
    "\n",
    "    for idx in indices:\n",
    "        example = dataset_split[idx]\n",
    "        judgment_tokens = len(analysis_tokenizer.encode(example['judgement']))\n",
    "        summary_tokens = len(analysis_tokenizer.encode(example['summary']))\n",
    "        judgment_lengths.append(judgment_tokens)\n",
    "        summary_lengths.append(summary_tokens)\n",
    "\n",
    "    return judgment_lengths, summary_lengths\n",
    "\n",
    "print(\"Analyzing token lengths (sampling 1000 examples)...\")\n",
    "judgment_lengths, summary_lengths = get_token_stats(dataset['train'], 1000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOKEN LENGTH STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nJudgment tokens:\")\n",
    "print(f\"  Mean: {np.mean(judgment_lengths):.0f}\")\n",
    "print(f\"  Median: {np.median(judgment_lengths):.0f}\")\n",
    "print(f\"  Min: {np.min(judgment_lengths)}\")\n",
    "print(f\"  Max: {np.max(judgment_lengths)}\")\n",
    "print(f\"  95th percentile: {np.percentile(judgment_lengths, 95):.0f}\")\n",
    "\n",
    "print(f\"\\nSummary tokens:\")\n",
    "print(f\"  Mean: {np.mean(summary_lengths):.0f}\")\n",
    "print(f\"  Median: {np.median(summary_lengths):.0f}\")\n",
    "print(f\"  Min: {np.min(summary_lengths)}\")\n",
    "print(f\"  Max: {np.max(summary_lengths)}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f8ced",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:36.423183Z",
     "iopub.status.busy": "2026-02-15T19:19:36.422881Z",
     "iopub.status.idle": "2026-02-15T19:19:36.445185Z",
     "shell.execute_reply": "2026-02-15T19:19:36.444425Z",
     "shell.execute_reply.started": "2026-02-15T19:19:36.423151Z"
    },
    "id": "d73f8ced",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampling dataset for GPU efficiency...\n",
      "Train: 4000, Validation: 500, Test: 300\n",
      "\n",
      "Available splits: ['train', 'test']\n",
      "\n",
      "\u26a0\ufe0f  No validation split found - creating from training data\n",
      "\u2713 Dataset subsampled successfully\n",
      "Final split sizes:\n",
      "  Train: 4000\n",
      "  Validation: 500\n",
      "  Test: 200\n"
     ]
    }
   ],
   "source": [
    "# Using 4000 training samples for optimal balance between performance and training time\n",
    "\n",
    "TRAIN_SAMPLES = 4000\n",
    "VAL_SAMPLES = 500\n",
    "TEST_SAMPLES = 300\n",
    "\n",
    "print(f\"Subsampling dataset for GPU efficiency...\")\n",
    "print(f\"Train: {TRAIN_SAMPLES}, Validation: {VAL_SAMPLES}, Test: {TEST_SAMPLES}\")\n",
    "print()\n",
    "\n",
    "# Set seed before sampling\n",
    "set_seed(SEED)\n",
    "\n",
    "# Check available splits\n",
    "print(f\"Available splits: {list(dataset.keys())}\")\n",
    "print()\n",
    "\n",
    "# Subsample each split - create validation from training data if needed\n",
    "train_dataset = dataset['train'].shuffle(seed=SEED).select(range(min(TRAIN_SAMPLES, len(dataset['train']))))\n",
    "\n",
    "# Create validation split from train data (since dataset might not have validation)\n",
    "if 'validation' in dataset:\n",
    "    val_dataset = dataset['validation'].shuffle(seed=SEED).select(range(min(VAL_SAMPLES, len(dataset['validation']))))\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No validation split found - creating from training data\")\n",
    "    # Use samples after training samples for validation\n",
    "    val_start = TRAIN_SAMPLES\n",
    "    val_end = val_start + VAL_SAMPLES\n",
    "    val_dataset = dataset['train'].shuffle(seed=SEED).select(range(val_start, min(val_end, len(dataset['train']))))\n",
    "\n",
    "test_dataset = dataset['test'].shuffle(seed=SEED).select(range(min(TEST_SAMPLES, len(dataset['test']))))\n",
    "\n",
    "print(\"\u2713 Dataset subsampled successfully\")\n",
    "print(f\"Final split sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)}\")\n",
    "print(f\"  Validation: {len(val_dataset)}\")\n",
    "print(f\"  Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e456e7c",
   "metadata": {
    "id": "7e456e7c"
   },
   "source": [
    "---\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "Convert the dataset into instruction-response format suitable for fine-tuning a generative model. We'll use a structured prompt template that clearly separates instruction, input, and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee30a5c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:36.446329Z",
     "iopub.status.busy": "2026-02-15T19:19:36.446070Z",
     "iopub.status.idle": "2026-02-15T19:19:36.451945Z",
     "shell.execute_reply": "2026-02-15T19:19:36.451358Z",
     "shell.execute_reply.started": "2026-02-15T19:19:36.446307Z"
    },
    "id": "ee30a5c4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruction Template:\n",
      "============================================================\n",
      "Instruction:\n",
      "Summarize the following legal court judgment.\n",
      "\n",
      "Input:\n",
      "[JUDGMENT TEXT]\n",
      "\n",
      "Response:\n",
      "[SUMMARY TEXT]\n",
      "============================================================\n",
      "\n",
      "Max judgment tokens: 1024\n",
      "Max summary tokens: 256\n",
      "Max total length: 1536\n"
     ]
    }
   ],
   "source": [
    "# Define instruction template for legal case summarization\n",
    "INSTRUCTION_TEMPLATE = \"\"\"Instruction:\n",
    "Summarize the following legal court judgment.\n",
    "\n",
    "Input:\n",
    "{judgement}\n",
    "\n",
    "Response:\n",
    "{summary}\"\"\"\n",
    "\n",
    "# Maximum token limits for model context window management\n",
    "MAX_JUDGMENT_TOKENS = 1024  # Truncate long judgments to fit in context\n",
    "MAX_SUMMARY_TOKENS = 256    # Maximum summary length\n",
    "MAX_TOTAL_LENGTH = 1536     # Total sequence length (judgment + summary + prompt)\n",
    "\n",
    "print(\"Instruction Template:\")\n",
    "print(\"=\"*60)\n",
    "print(INSTRUCTION_TEMPLATE.format(\n",
    "    judgement=\"[JUDGMENT TEXT]\",\n",
    "    summary=\"[SUMMARY TEXT]\"\n",
    "))\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nMax judgment tokens: {MAX_JUDGMENT_TOKENS}\")\n",
    "print(f\"Max summary tokens: {MAX_SUMMARY_TOKENS}\")\n",
    "print(f\"Max total length: {MAX_TOTAL_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b90a244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:36.453174Z",
     "iopub.status.busy": "2026-02-15T19:19:36.452914Z",
     "iopub.status.idle": "2026-02-15T19:19:39.249421Z",
     "shell.execute_reply": "2026-02-15T19:19:39.248731Z",
     "shell.execute_reply.started": "2026-02-15T19:19:36.453143Z"
    },
    "id": "9b90a244",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: google/gemma-2b\n",
      "\u2713 Tokenizer loaded successfully\n",
      "Vocabulary size: 256000\n",
      "Pad token: '<pad>' (ID: 0)\n",
      "EOS token: '<eos>' (ID: 1)\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "MODEL_NAME = \"google/gemma-2b\"\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(\"\u2713 Tokenizer loaded successfully\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: '{tokenizer.pad_token}' (ID: {tokenizer.pad_token_id})\")\n",
    "print(f\"EOS token: '{tokenizer.eos_token}' (ID: {tokenizer.eos_token_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2f198c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:39.250615Z",
     "iopub.status.busy": "2026-02-15T19:19:39.250326Z",
     "iopub.status.idle": "2026-02-15T19:19:39.257056Z",
     "shell.execute_reply": "2026-02-15T19:19:39.256502Z",
     "shell.execute_reply.started": "2026-02-15T19:19:39.250592Z"
    },
    "id": "d2f198c7",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Preprocessing functions defined\n"
     ]
    }
   ],
   "source": [
    "def truncate_text_by_tokens(text, max_tokens, tokenizer):\n",
    "    \"\"\"\n",
    "    Truncate text to a maximum number of tokens.\n",
    "    This ensures we don't exceed the model's context window.\n",
    "    \"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Format a single example into instruction-response format.\n",
    "    Truncates judgment to fit within token limits.\n",
    "    \"\"\"\n",
    "    # Truncate judgment to max tokens\n",
    "    truncated_judgment = truncate_text_by_tokens(\n",
    "        example['judgement'],\n",
    "        MAX_JUDGMENT_TOKENS,\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    # Truncate summary to max tokens\n",
    "    truncated_summary = truncate_text_by_tokens(\n",
    "        example['summary'],\n",
    "        MAX_SUMMARY_TOKENS,\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    # Format into instruction template\n",
    "    formatted_text = INSTRUCTION_TEMPLATE.format(\n",
    "        judgement=truncated_judgment,\n",
    "        summary=truncated_summary\n",
    "    )\n",
    "\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize preprocessed examples with padding and truncation.\n",
    "    \"\"\"\n",
    "    # Tokenize with truncation and padding\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_TOTAL_LENGTH,\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    # Create labels (copy of input_ids for causal language modeling)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "print(\"\u2713 Preprocessing functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fb80b72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:19:39.258085Z",
     "iopub.status.busy": "2026-02-15T19:19:39.257890Z",
     "iopub.status.idle": "2026-02-15T19:21:16.985151Z",
     "shell.execute_reply": "2026-02-15T19:21:16.984307Z",
     "shell.execute_reply.started": "2026-02-15T19:19:39.258066Z"
    },
    "id": "6fb80b72",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting datasets into instruction-response format...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8752aed1ef7a4b7ebf40503b54fd4cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daff1b95cd424051a099472ed7f75e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090f3dc4b1ba404e99133fa1e5d15ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Datasets formatted\n",
      "\n",
      "Sample formatted example:\n",
      "============================================================\n",
      "Instruction:\n",
      "Summarize the following legal court judgment.\n",
      "\n",
      "Input:\n",
      "Appeal No. 945 of 1965.\n",
      "Appeal by special leave from the judgment and order dated December 14, 1962 of the Gujarat High Court in Sales Tax Re ference No. 16 of 1961.\n",
      "N. section Bindra and R. H. Dhebar, for the appellant.\n",
      "M. V. Goswami, for the respondent.\n",
      "The Judgment of the Court was delivered by Bhargava, J.\n",
      "This appeal under special leave granted by this Court arises out of proceedings for assessment of sales tax under the Bombay Sales Tax Act III of 1953.\n",
      "Messrs. Kailash Engineering Co. (hereinafter referred to as \"the respondent\") was an engineering concern having their workshop at Morvi on the meter gauge section of the Western Railway.\n",
      "They obtained a contract from the Western Railway Administration for construction ...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Apply formatting to datasets\n",
    "print(\"Formatting datasets into instruction-response format...\")\n",
    "print()\n",
    "\n",
    "# Format each split\n",
    "train_formatted = train_dataset.map(format_instruction, remove_columns=train_dataset.column_names)\n",
    "val_formatted = val_dataset.map(format_instruction, remove_columns=val_dataset.column_names)\n",
    "test_formatted = test_dataset.map(format_instruction, remove_columns=test_dataset.column_names)\n",
    "\n",
    "print(\"\u2713 Datasets formatted\")\n",
    "print()\n",
    "print(\"Sample formatted example:\")\n",
    "print(\"=\"*60)\n",
    "print(train_formatted[0]['text'][:800] + \"...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c987bb25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:21:16.986525Z",
     "iopub.status.busy": "2026-02-15T19:21:16.986238Z",
     "iopub.status.idle": "2026-02-15T19:21:38.193571Z",
     "shell.execute_reply": "2026-02-15T19:21:38.192845Z",
     "shell.execute_reply.started": "2026-02-15T19:21:16.986500Z"
    },
    "id": "c987bb25",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132f40cd19ac4443b539d5aa146bf0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train set:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083183d206c94f9ab50435d414e23fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing validation set:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa4a4103866c4df49dee5e8cd838e546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing test set:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Tokenization complete\n",
      "\n",
      "Tokenized dataset features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
      "Example input_ids shape: 1536\n"
     ]
    }
   ],
   "source": [
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "print()\n",
    "\n",
    "train_tokenized = train_formatted.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_formatted.column_names,\n",
    "    desc=\"Tokenizing train set\"\n",
    ")\n",
    "\n",
    "val_tokenized = val_formatted.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_formatted.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "test_tokenized = test_formatted.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=test_formatted.column_names,\n",
    "    desc=\"Tokenizing test set\"\n",
    ")\n",
    "\n",
    "print(\"\u2713 Tokenization complete\")\n",
    "print(f\"\\nTokenized dataset features: {train_tokenized.features}\")\n",
    "print(f\"Example input_ids shape: {len(train_tokenized[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a657753",
   "metadata": {
    "id": "9a657753"
   },
   "source": [
    "---\n",
    "## 4. Model Selection and Configuration\n",
    "\n",
    "Load Gemma-2B model with 4-bit quantization for memory-efficient training on Colab's free GPU. We use QLoRA (Quantized LoRA) to enable fine-tuning on consumer hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24852cdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:21:38.194952Z",
     "iopub.status.busy": "2026-02-15T19:21:38.194620Z",
     "iopub.status.idle": "2026-02-15T19:21:38.200329Z",
     "shell.execute_reply": "2026-02-15T19:21:38.199620Z",
     "shell.execute_reply.started": "2026-02-15T19:21:38.194921Z"
    },
    "id": "24852cdf",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4-bit Quantization Configuration:\n",
      "============================================================\n",
      "Quantization type: NF4 (4-bit NormalFloat)\n",
      "Compute dtype: bfloat16\n",
      "Double quantization: Enabled\n",
      "Expected memory reduction: ~75% vs fp16\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure 4-bit quantization for memory efficiency\n",
    "# This reduces memory footprint by ~75% compared to fp16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                      # Enable 4-bit quantization\n",
    "    bnb_4bit_quant_type=\"nf4\",             # Use NormalFloat4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16 for stability\n",
    "    bnb_4bit_use_double_quant=True,        # Double quantization for extra compression\n",
    ")\n",
    "\n",
    "print(\"4-bit Quantization Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Quantization type: NF4 (4-bit NormalFloat)\")\n",
    "print(f\"Compute dtype: bfloat16\")\n",
    "print(f\"Double quantization: Enabled\")\n",
    "print(f\"Expected memory reduction: ~75% vs fp16\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83652419",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:21:38.201349Z",
     "iopub.status.busy": "2026-02-15T19:21:38.201162Z",
     "iopub.status.idle": "2026-02-15T19:22:06.162932Z",
     "shell.execute_reply": "2026-02-15T19:22:06.162193Z",
     "shell.execute_reply.started": "2026-02-15T19:21:38.201331Z"
    },
    "id": "83652419",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: google/gemma-2b\n",
      "This will take 2-3 minutes...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4923fe58ebbe467296098901a92d4541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5038880d834449aaa61c5a583904747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12515ee974040b890f2d803576077ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7b016090a434e80907e25165dd9d4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4106737b020044eeb71c2742eadb1af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Model loaded successfully\n",
      "\n",
      "Model Configuration:\n",
      "============================================================\n",
      "Model name: google/gemma-2b\n",
      "Parameters: 2.51B\n",
      "Gradient checkpointing: Enabled\n",
      "Device: cuda:0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Load base model with quantization\n",
    "print(f\"Loading base model: {MODEL_NAME}\")\n",
    "print(\"This will take 2-3 minutes...\")\n",
    "print()\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",                    # Automatically distribute across GPUs\n",
    "    trust_remote_code=True,               # Required for Gemma\n",
    ")\n",
    "\n",
    "# Enable gradient checkpointing to reduce memory during training\n",
    "# Trade-off: ~20% slower training for ~30% memory reduction\n",
    "base_model.config.use_cache = False\n",
    "base_model.gradient_checkpointing_enable()\n",
    "\n",
    "print(\"\u2713 Model loaded successfully\")\n",
    "print()\n",
    "print(\"Model Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model name: {MODEL_NAME}\")\n",
    "print(f\"Parameters: {base_model.num_parameters() / 1e9:.2f}B\")\n",
    "print(f\"Gradient checkpointing: Enabled\")\n",
    "print(f\"Device: {next(base_model.parameters()).device}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8d6ac",
   "metadata": {
    "id": "81f8d6ac"
   },
   "source": [
    "---\n",
    "## 5. Apply LoRA Using PEFT\n",
    "\n",
    "Configure and apply Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning. LoRA only trains a small subset of parameters (typically <1% of total parameters) while achieving comparable performance to full fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "da770ae4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:22:06.164622Z",
     "iopub.status.busy": "2026-02-15T19:22:06.163921Z",
     "iopub.status.idle": "2026-02-15T19:22:06.190910Z",
     "shell.execute_reply": "2026-02-15T19:22:06.190286Z",
     "shell.execute_reply.started": "2026-02-15T19:22:06.164595Z"
    },
    "id": "da770ae4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Model prepared for k-bit training\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "print(\"\u2713 Model prepared for k-bit training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "553ee431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:22:06.192113Z",
     "iopub.status.busy": "2026-02-15T19:22:06.191823Z",
     "iopub.status.idle": "2026-02-15T19:22:07.942044Z",
     "shell.execute_reply": "2026-02-15T19:22:07.941119Z",
     "shell.execute_reply.started": "2026-02-15T19:22:06.192081Z"
    },
    "id": "553ee431",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Configuration:\n",
      "============================================================\n",
      "Rank (r): 16\n",
      "  \u2192 Controls the dimensionality of low-rank matrices\n",
      "  \u2192 Higher rank = more expressiveness but more parameters\n",
      "\n",
      "Alpha: 16\n",
      "  \u2192 Scaling factor for LoRA updates\n",
      "  \u2192 Typically set equal to r for balanced scaling\n",
      "\n",
      "Target modules: {'q_proj', 'v_proj'}\n",
      "  \u2192 Attention query and value projections\n",
      "  \u2192 These are most impactful for adaptation\n",
      "\n",
      "Dropout: 0.05\n",
      "  \u2192 Prevents overfitting on small dataset\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA parameters\n",
    "# These hyperparameters balance training efficiency with model expressiveness\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                                  # Rank of update matrices (higher = more parameters)\n",
    "    lora_alpha=16,                         # Scaling factor (typically set equal to r)\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Apply LoRA to query and value projection layers\n",
    "    lora_dropout=0.05,                     # Dropout for regularization\n",
    "    bias=\"none\",                           # Don't train bias parameters\n",
    "    task_type=\"CAUSAL_LM\"                  # Task type: Causal Language Modeling\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Rank (r): {lora_config.r}\")\n",
    "print(f\"  \u2192 Controls the dimensionality of low-rank matrices\")\n",
    "print(f\"  \u2192 Higher rank = more expressiveness but more parameters\")\n",
    "print()\n",
    "print(f\"Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"  \u2192 Scaling factor for LoRA updates\")\n",
    "print(f\"  \u2192 Typically set equal to r for balanced scaling\")\n",
    "print()\n",
    "print(f\"Target modules: {lora_config.target_modules}\")\n",
    "print(f\"  \u2192 Attention query and value projections\")\n",
    "print(f\"  \u2192 These are most impactful for adaptation\")\n",
    "print()\n",
    "print(f\"Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"  \u2192 Prevents overfitting on small dataset\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d945f83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:22:07.943610Z",
     "iopub.status.busy": "2026-02-15T19:22:07.943253Z",
     "iopub.status.idle": "2026-02-15T19:22:10.452483Z",
     "shell.execute_reply": "2026-02-15T19:22:10.451741Z",
     "shell.execute_reply.started": "2026-02-15T19:22:07.943575Z"
    },
    "id": "1d945f83",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 LoRA applied to model\n",
      "\n",
      "Parameter Analysis:\n",
      "============================================================\n",
      "Total parameters: 1,517,111,296\n",
      "Trainable parameters: 1,843,200\n",
      "Trainable percentage: 0.1215%\n",
      "\n",
      "Memory efficiency: Training only 0.12% of parameters!\n",
      "============================================================\n",
      "trainable params: 1,843,200 || all params: 2,508,015,616 || trainable%: 0.0735\n"
     ]
    }
   ],
   "source": [
    "# Apply LoRA to the model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "print(\"\u2713 LoRA applied to model\")\n",
    "print()\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percent = 100 * trainable_params / total_params\n",
    "\n",
    "print(\"Parameter Analysis:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Trainable percentage: {trainable_percent:.4f}%\")\n",
    "print()\n",
    "print(f\"Memory efficiency: Training only {trainable_percent:.2f}% of parameters!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Print model summary\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff838f8",
   "metadata": {
    "id": "5ff838f8"
   },
   "source": [
    "---\n",
    "## 6. Training Setup\n",
    "\n",
    "Configure training hyperparameters optimized for Colab's T4 GPU. We use gradient accumulation to simulate larger batch sizes without OOM errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3442996",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:39:31.189860Z",
     "iopub.status.busy": "2026-02-15T19:39:31.189210Z",
     "iopub.status.idle": "2026-02-15T19:39:31.225958Z",
     "shell.execute_reply": "2026-02-15T19:39:31.225186Z",
     "shell.execute_reply.started": "2026-02-15T19:39:31.189828Z"
    },
    "id": "c3442996",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "warmup_ratio is deprecated and will be removed in v5.2. Use `warmup_steps` instead.\n",
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "============================================================\n",
      "Learning Rate: 0.0002\n",
      "  \u2192 Higher LR (2e-4) suitable for LoRA small parameter space\n",
      "\n",
      "Epochs: 2\n",
      "  \u2192 2 epochs sufficient to avoid overfitting on domain data\n",
      "\n",
      "Batch Size per Device: 2\n",
      "  \u2192 Small to fit in T4 GPU memory (16GB)\n",
      "\n",
      "Gradient Accumulation Steps: 8\n",
      "  \u2192 Effective batch size: 16\n",
      "  \u2192 Larger effective batch = more stable gradients\n",
      "\n",
      "Mixed Precision (FP16): True\n",
      "  \u2192 2x faster training, 50% memory reduction\n",
      "\n",
      "LR Scheduler: SchedulerType.COSINE\n",
      "  \u2192 Cosine annealing for smooth convergence\n",
      "\n",
      "Optimizer: OptimizerNames.PAGED_ADAMW_8BIT\n",
      "  \u2192 8-bit AdamW reduces optimizer memory by 75%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Define training hyperparameters\n",
    "# Each parameter is carefully chosen for optimal training on Colab GPU\n",
    "\n",
    "OUTPUT_DIR = \"./legal-summarization-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # Output and logging\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,                      # Log every 50 steps for monitoring\n",
    "\n",
    "    # Training schedule\n",
    "    num_train_epochs=2,                    # 2 epochs sufficient for LoRA fine-tuning\n",
    "    learning_rate=2e-4,                    # Higher LR works well with LoRA\n",
    "    lr_scheduler_type=\"cosine\",            # Cosine annealing for smooth convergence\n",
    "    warmup_ratio=0.05,                     # 5% warmup steps\n",
    "\n",
    "    # Batch size and accumulation\n",
    "    per_device_train_batch_size=2,         # Small batch size to fit in 16GB GPU\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,         # Effective batch size = 2 * 8 = 16\n",
    "\n",
    "    # Mixed precision and optimization\n",
    "    fp16=True,                             # Use mixed precision for speed\n",
    "    optim=\"paged_adamw_8bit\",             # Memory-efficient optimizer\n",
    "\n",
    "    # Evaluation and saving\n",
    "    eval_strategy=\"epoch\",                 # Evaluate at end of each epoch\n",
    "    save_strategy=\"epoch\",                 # Save checkpoint each epoch\n",
    "    save_total_limit=2,                    # Keep only 2 best checkpoints\n",
    "    load_best_model_at_end=True,          # Load best model after training\n",
    "    metric_for_best_model=\"eval_loss\",    # Use validation loss for model selection\n",
    "\n",
    "    # Performance\n",
    "    dataloader_num_workers=2,              # Parallel data loading\n",
    "    dataloader_pin_memory=True,            # Faster GPU transfer\n",
    "\n",
    "    # Miscellaneous\n",
    "    report_to=\"none\",                      # Disable wandb/tensorboard\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Learning Rate: {training_args.learning_rate}\")\n",
    "print(f\"  \u2192 Higher LR (2e-4) suitable for LoRA small parameter space\")\n",
    "print()\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  \u2192 2 epochs sufficient to avoid overfitting on domain data\")\n",
    "print()\n",
    "print(f\"Batch Size per Device: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  \u2192 Small to fit in T4 GPU memory (16GB)\")\n",
    "print()\n",
    "print(f\"Gradient Accumulation Steps: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  \u2192 Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"  \u2192 Larger effective batch = more stable gradients\")\n",
    "print()\n",
    "print(f\"Mixed Precision (FP16): {training_args.fp16}\")\n",
    "print(f\"  \u2192 2x faster training, 50% memory reduction\")\n",
    "print()\n",
    "print(f\"LR Scheduler: {training_args.lr_scheduler_type}\")\n",
    "print(f\"  \u2192 Cosine annealing for smooth convergence\")\n",
    "print()\n",
    "print(f\"Optimizer: {training_args.optim}\")\n",
    "print(f\"  \u2192 8-bit AdamW reduces optimizer memory by 75%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "694a9b32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:39:41.131283Z",
     "iopub.status.busy": "2026-02-15T19:39:41.130970Z",
     "iopub.status.idle": "2026-02-15T19:39:41.135681Z",
     "shell.execute_reply": "2026-02-15T19:39:41.135070Z",
     "shell.execute_reply.started": "2026-02-15T19:39:41.131257Z"
    },
    "id": "694a9b32",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Data collator created\n"
     ]
    }
   ],
   "source": [
    "# Create data collator for causal language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal LM, not masked LM\n",
    ")\n",
    "\n",
    "print(\"\u2713 Data collator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92926eca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:39:44.101278Z",
     "iopub.status.busy": "2026-02-15T19:39:44.100553Z",
     "iopub.status.idle": "2026-02-15T19:39:44.115565Z",
     "shell.execute_reply": "2026-02-15T19:39:44.115036Z",
     "shell.execute_reply.started": "2026-02-15T19:39:44.101248Z"
    },
    "id": "92926eca",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Trainer initialized\n",
      "\n",
      "Training samples: 4000\n",
      "Validation samples: 500\n",
      "Steps per epoch: 250\n",
      "Total training steps: 500\n"
     ]
    }
   ],
   "source": [
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Trainer initialized\")\n",
    "print()\n",
    "print(f\"Training samples: {len(train_tokenized)}\")\n",
    "print(f\"Validation samples: {len(val_tokenized)}\")\n",
    "print(f\"Steps per epoch: {len(train_tokenized) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)}\")\n",
    "print(f\"Total training steps: {len(train_tokenized) // (training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps) * training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6caec09e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-15T19:39:48.327282Z",
     "iopub.status.busy": "2026-02-15T19:39:48.326421Z",
     "iopub.status.idle": "2026-02-16T06:15:42.681487Z",
     "shell.execute_reply": "2026-02-16T06:15:42.680528Z",
     "shell.execute_reply.started": "2026-02-15T19:39:48.327255Z"
    },
    "id": "6caec09e",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "Training will take approximately 30-45 minutes on Colab T4 GPU\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 10:34:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.809841</td>\n",
       "      <td>1.804943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.786816</td>\n",
       "      <td>1.795600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE\n",
      "============================================================\n",
      "Training time: 635.90 minutes\n",
      "Peak GPU memory: 14.15 GB\n",
      "Final training loss: 1.8199\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Start training with time and memory tracking\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "print(\"Training will take approximately 30-45 minutes on Colab T4 GPU\")\n",
    "print()\n",
    "\n",
    "# Clear GPU cache before training\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Record start time\n",
    "start_time = time.time()\n",
    "start_memory = torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "\n",
    "# Train the model\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Record end time and memory\n",
    "end_time = time.time()\n",
    "peak_memory = torch.cuda.max_memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "training_time = end_time - start_time\n",
    "\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training time: {training_time / 60:.2f} minutes\")\n",
    "print(f\"Peak GPU memory: {peak_memory:.2f} GB\")\n",
    "print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b42e65e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:15:42.683799Z",
     "iopub.status.busy": "2026-02-16T06:15:42.683424Z",
     "iopub.status.idle": "2026-02-16T06:15:43.376794Z",
     "shell.execute_reply": "2026-02-16T06:15:43.375976Z",
     "shell.execute_reply.started": "2026-02-16T06:15:42.683769Z"
    },
    "id": "b42e65e0",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fine-tuned model...\n",
      "\u2713 Model saved to ./legal-summarization-lora\n",
      "\n",
      "Saved files:\n",
      "  - tokenizer.json\n",
      "  - adapter_config.json\n",
      "  - tokenizer_config.json\n",
      "  - README.md\n",
      "  - adapter_model.safetensors\n"
     ]
    }
   ],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"Saving fine-tuned model...\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\u2713 Model saved to {OUTPUT_DIR}\")\n",
    "print()\n",
    "print(\"Saved files:\")\n",
    "import os\n",
    "for file in os.listdir(OUTPUT_DIR):\n",
    "    if not file.startswith('checkpoint'):\n",
    "        print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90794ba",
   "metadata": {
    "id": "c90794ba"
   },
   "source": [
    "---\n",
    "## 7. Evaluation\n",
    "\n",
    "Evaluate the fine-tuned model using ROUGE metrics and compare against the base pre-trained model (zero-shot). ROUGE scores measure n-gram overlap between generated and reference summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88cd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "!pip install -q evaluate rouge_score bert_score huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fab3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Model path found: /kaggle/input/datasets/orpheusmanga/legal-summarization-lora1\n",
      "Configuration:\n",
      "  Model path: /kaggle/input/datasets/orpheusmanga/legal-summarization-lora1\n",
      "  Base model: google/gemma-2b\n",
      "  Test samples: 100\n"
     ]
    }
   ],
   "source": [
    "# Configuration - UPDATE THESE PATHS\n",
    "MODEL_PATH = \"/kaggle/input/datasets/orpheusmanga/legal-summarization-lora\"  # Path to your trained model\n",
    "BASE_MODEL_NAME = \"google/gemma-2b\"       # Base model used for training\n",
    "MAX_JUDGMENT_TOKENS = 1024\n",
    "MAX_NEW_TOKENS = 256\n",
    "NUM_TEST_SAMPLES = 100\n",
    "\n",
    "# Verify model path exists\n",
    "import os\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    print(f\"\u26a0\ufe0f  WARNING: Model path not found: {MODEL_PATH}\")\n",
    "    print(\"Please update MODEL_PATH to point to your trained model folder\")\n",
    "    print(\"It should contain: adapter_model.safetensors, adapter_config.json, tokenizer files\")\n",
    "else:\n",
    "    print(f\"\u2713 Model path found: {MODEL_PATH}\")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model path: {MODEL_PATH}\")\n",
    "print(f\"  Base model: {BASE_MODEL_NAME}\")\n",
    "print(f\"  Test samples: {NUM_TEST_SAMPLES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af070c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING MODEL FILES\n",
      "============================================================\n",
      "\n",
      "Files found in /kaggle/input/datasets/orpheusmanga/legal-summarization-lora1:\n",
      "  \u2713 adapter_config.json\n",
      "  \u2713 adapter_model.safetensors\n",
      "  \u2713 tokenizer.json\n",
      "  \u2713 tokenizer_config.json\n",
      "\n",
      "Required files check:\n",
      "  \u2713 adapter_config.json\n",
      "  \u2713 adapter_model.safetensors\n",
      "  \u2713 tokenizer_config.json\n",
      "  \u2713 tokenizer.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify model files\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING MODEL FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "required_files = [\n",
    "    'adapter_config.json',\n",
    "    'adapter_model.safetensors',  # or adapter_model.bin\n",
    "    'tokenizer_config.json',\n",
    "    'tokenizer.json'\n",
    "]\n",
    "\n",
    "import os\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    files = os.listdir(MODEL_PATH)\n",
    "    print(f\"\\nFiles found in {MODEL_PATH}:\")\n",
    "    for f in sorted(files):\n",
    "        print(f\"  \u2713 {f}\")\n",
    "    \n",
    "    # Check for required files\n",
    "    print(\"\\nRequired files check:\")\n",
    "    for req_file in required_files:\n",
    "        if req_file in files or (req_file == 'adapter_model.safetensors' and 'adapter_model.bin' in files):\n",
    "            print(f\"  \u2713 {req_file}\")\n",
    "        else:\n",
    "            print(f\"  \u26a0\ufe0f  Missing: {req_file}\")\n",
    "else:\n",
    "    print(f\"\\n\u274c Path does not exist: {MODEL_PATH}\")\n",
    "    print(\"Please update MODEL_PATH in the configuration cell above\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d246a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Loaded 100 test samples\n",
      "  Dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "  First example keys: ['judgement', 'dataset_name', 'summary']\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset\n",
    "print(\"Loading test dataset...\")\n",
    "dataset = load_dataset(\"joelniklaus/legal_case_document_summarization\", \"default\")\n",
    "test_dataset = dataset['test'].shuffle(seed=42).select(range(NUM_TEST_SAMPLES))\n",
    "print(f\"\u2713 Loaded {len(test_dataset)} test samples\")\n",
    "print(f\"  Dataset type: {type(test_dataset)}\")\n",
    "print(f\"  First example keys: {list(test_dataset[0].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cce5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapters from local trained model...\n",
      "  Path: /kaggle/input/datasets/orpheusmanga/legal-summarization-lora1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/peft/config.py:165: UserWarning: Unexpected keyword arguments ['alora_invocation_tokens', 'arrow_config', 'ensure_weight_tying', 'peft_version'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Fine-tuned model loaded successfully!\n",
      "  Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load your trained LoRA adapters\n",
    "print(\"Loading LoRA adapters from local trained model...\")\n",
    "print(f\"  Path: {MODEL_PATH}\")\n",
    "\n",
    "# Load adapters from local directory\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    MODEL_PATH,\n",
    "    is_trainable=False  # Set to evaluation mode\n",
    ")\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"\u2713 Fine-tuned model loaded successfully!\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0827f4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation metrics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5d71207141433990aed0dffca150e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf7165c508b43f380dd07d9f6a7cedf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50715d82b2642aa9a5daf5a710c0e8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abea2c0715d64c65aaf0f33cc7afc793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59311daec1842138dcd374b83198b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Metrics loaded\n"
     ]
    }
   ],
   "source": [
    "# Load evaluation metrics\n",
    "print(\"Loading evaluation metrics...\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "print(\"\u2713 Metrics loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6953ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "# Define helper functions\n",
    "def truncate_text_by_tokens(text, max_tokens, tokenizer):\n",
    "    \"\"\"Truncate text to max tokens\"\"\"\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "def generate_summary(model, tokenizer, judgment_text, max_new_tokens=256):\n",
    "    \"\"\"Generate summary for a legal judgment\"\"\"\n",
    "    prompt = f\"\"\"Instruction:\n",
    "Summarize the following legal court judgment.\n",
    "\n",
    "Input:\n",
    "{judgment_text}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1280).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    if \"Response:\" in generated_text:\n",
    "        summary = generated_text.split(\"Response:\")[-1].strip()\n",
    "    else:\n",
    "        summary = generated_text[len(prompt):].strip()\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"\u2713 Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1216772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING EVALUATION\n",
      "============================================================\n",
      "\n",
      "Evaluating 100 samples...\n",
      "This will take 10-15 minutes\n",
      "\n",
      "Progress: 0/100\n",
      "Progress: 20/100\n",
      "Progress: 40/100\n",
      "Progress: 60/100\n",
      "Progress: 80/100\n",
      "\n",
      "\u2713 Generated 100 summaries\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nEvaluating {NUM_TEST_SAMPLES} samples...\")\n",
    "print(\"This will take 10-15 minutes\\n\")\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "perplexities = []\n",
    "\n",
    "for i in range(NUM_TEST_SAMPLES):\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Progress: {i}/{NUM_TEST_SAMPLES}\")\n",
    "    \n",
    "    # Get example and check structure\n",
    "    example = test_dataset[i]\n",
    "    \n",
    "    # Handle both dict and dataset row formats\n",
    "    if isinstance(example, dict):\n",
    "        judgment_text = example['judgement']\n",
    "        reference_summary = example['summary']\n",
    "    else:\n",
    "        # If it's a dataset row, access columns properly\n",
    "        judgment_text = example['judgement'] if 'judgement' in example else str(example)\n",
    "        reference_summary = example['summary'] if 'summary' in example else \"\"\n",
    "    \n",
    "    judgment = truncate_text_by_tokens(judgment_text, MAX_JUDGMENT_TOKENS, tokenizer)\n",
    "    \n",
    "    # Generate summary\n",
    "    pred_summary = generate_summary(model, tokenizer, judgment, MAX_NEW_TOKENS)\n",
    "    \n",
    "    predictions.append(pred_summary)\n",
    "    references.append(reference_summary)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    try:\n",
    "        inputs = tokenizer(pred_summary, return_tensors=\"pt\", truncation=True, max_length=512).to(model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss.item()\n",
    "            perplexity = torch.exp(torch.tensor(loss)).item()\n",
    "            perplexities.append(perplexity)\n",
    "    except Exception as e:\n",
    "        if i == 0:  # Only print error for first sample\n",
    "            print(f\"Warning: Could not compute perplexity: {e}\")\n",
    "        pass\n",
    "\n",
    "print(f\"\\n\u2713 Generated {len(predictions)} summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7779513b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing metrics...\n",
      "Predictions: 100, References: 100\n",
      "Computing ROUGE...\n",
      "Computing BLEU...\n",
      "Computing BERTScore (this takes a few minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb042538c8df4422ad91e34704fdddde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead22a10632743bcbbf33127e7a3c288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "631c2e7d67ef44308a6df4810aaaaaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adf48578ca4d409ebab0bbf5c25b8abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2444f9c501bc4a6ea66fd9d9db0ed66f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore type: <class 'dict'>\n",
      "BERTScore keys: dict_keys(['precision', 'recall', 'f1', 'hashcode'])\n",
      "\u2713 All metrics computed\n"
     ]
    }
   ],
   "source": [
    "# Compute metrics\n",
    "print(\"\\nComputing metrics...\")\n",
    "print(f\"Predictions: {len(predictions)}, References: {len(references)}\")\n",
    "\n",
    "# Ensure all strings are valid\n",
    "predictions_clean = [str(p) if p else \"\" for p in predictions]\n",
    "references_clean = [str(r) if r else \"\" for r in references]\n",
    "\n",
    "# ROUGE\n",
    "print(\"Computing ROUGE...\")\n",
    "rouge_results = rouge.compute(\n",
    "    predictions=predictions_clean,\n",
    "    references=references_clean,\n",
    "    use_stemmer=True\n",
    ")\n",
    "\n",
    "# BLEU\n",
    "print(\"Computing BLEU...\")\n",
    "bleu_references = [[ref] for ref in references_clean]\n",
    "bleu_results = bleu.compute(\n",
    "    predictions=predictions_clean,\n",
    "    references=bleu_references\n",
    ")\n",
    "\n",
    "# BERTScore\n",
    "print(\"Computing BERTScore (this takes a few minutes)...\")\n",
    "try:\n",
    "    bertscore_results = bertscore.compute(\n",
    "        predictions=predictions_clean,\n",
    "        references=references_clean,\n",
    "        model_type=\"distilbert-base-uncased\",\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"BERTScore type: {type(bertscore_results)}\")\n",
    "    if isinstance(bertscore_results, dict):\n",
    "        print(f\"BERTScore keys: {bertscore_results.keys()}\")\n",
    "except Exception as e:\n",
    "    print(f\"BERTScore error: {e}\")\n",
    "    # Fallback values if BERTScore fails\n",
    "    bertscore_results = {\n",
    "        'precision': [0.0] * len(predictions_clean),\n",
    "        'recall': [0.0] * len(predictions_clean),\n",
    "        'f1': [0.0] * len(predictions_clean)\n",
    "    }\n",
    "\n",
    "# Perplexity\n",
    "avg_perplexity = np.mean(perplexities) if perplexities else None\n",
    "\n",
    "print(\"\u2713 All metrics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e23caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "      Metric  Score                  Interpretation\n",
      "     ROUGE-1 0.2908             Vocabulary coverage\n",
      "     ROUGE-2 0.1341                  Phrase fluency\n",
      "     ROUGE-L 0.1760            Structural coherence\n",
      "        BLEU 0.0057         Precision-based overlap\n",
      " BERTScore-P 0.8218              Semantic precision\n",
      " BERTScore-R 0.7614                 Semantic recall\n",
      "BERTScore-F1 0.7902             Semantic F1 balance\n",
      "  Perplexity   6.80 Model confidence (lower=better)\n",
      "\n",
      "============================================================\n",
      "\n",
      "\u2713 Results saved to evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'BLEU', 'BERTScore-P', 'BERTScore-R', 'BERTScore-F1', 'Perplexity'],\n",
    "    'Score': [\n",
    "        f\"{rouge_results['rouge1']:.4f}\",\n",
    "        f\"{rouge_results['rouge2']:.4f}\",\n",
    "        f\"{rouge_results['rougeL']:.4f}\",\n",
    "        f\"{bleu_results['bleu']:.4f}\",\n",
    "        f\"{np.mean(bertscore_results['precision']):.4f}\",\n",
    "        f\"{np.mean(bertscore_results['recall']):.4f}\",\n",
    "        f\"{np.mean(bertscore_results['f1']):.4f}\",\n",
    "        f\"{avg_perplexity:.2f}\" if avg_perplexity else \"N/A\"\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'Vocabulary coverage',\n",
    "        'Phrase fluency',\n",
    "        'Structural coherence',\n",
    "        'Precision-based overlap',\n",
    "        'Semantic precision',\n",
    "        'Semantic recall',\n",
    "        'Semantic F1 balance',\n",
    "        'Model confidence (lower=better)'\n",
    "    ]\n",
    "}\n",
    ")\n",
    "\n",
    "print(results_df.to_string(index=False))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('evaluation_results.csv', index=False)\n",
    "print(\"\\n\u2713 Results saved to evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba490b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 1\n",
      "============================================================\n",
      "\n",
      "Input (first 200 chars):\n",
      "The appellant brought a claim for judicial review of a decision of the respondent, on 21 February 2012, to approve a Revenue Budget for 2012/13 in relation to the provision of youth services.\n",
      "In his c...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Reference Summary:\n",
      "Mr Aaron Hunt, born on 17 April 1991, suffers from ADHD, learning difficulties and behavioural problems.\n",
      "As a result, North Somerset Council (the Council) are statutorily required, so far as reasonably practicable, to secure access for him to sufficient educational and recreational leisure time activities for the improvement of his well being.\n",
      "On 21 February 2012, the Council made a decision to ap...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Generated Summary:\n",
      "The appellant was a young person with a disability who used to attend a weekly youth club.\n",
      "He was concerned about the impact which the reduction in the youth services budget was likely to have on the provision of services for young persons with disabilities and in particular on a weekly youth club for vulnerable young people which he used to attend.\n",
      "The appellant brought a claim for judicial revie...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 2\n",
      "============================================================\n",
      "\n",
      "Input (first 200 chars):\n",
      "Appeal No. 1940 of 1967.\n",
      "Appeal by special leave from the judgment and order dated April 17, 27, 1967 of the Gujarat High Court in Civil Revision Application 328 of 1967.\n",
      "section T. Desai and I. N. Sh...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Reference Summary:\n",
      "R and F who held a cinema building in Ahmedabad on lease entered on November 27, 1954 into an agreement with respondent No. 1 giving the latter a right to exhibit cinematograph films in the said building.\n",
      "Later respondent No. 1 filed 'suit No. 149 of 1960 to assert his right to exhibit films in the building.\n",
      "The suit resulted in a compromise decree.\n",
      "In pursuance of the compromise a further agreeme...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Generated Summary:\n",
      "On August 19, 1954, the receivers of the owners of a building granted a lease of the building to two persons, Raval and Faraqui, and by an agreement dated November 27, 1954, between Raval and Faraqui and \"Filmistan\" (a company) on the one hand and the \"receivers\" on the other hand, right to exhibit cinematograph films was granted to the latter on certain terms and conditions.\n",
      "By an order dated Dec...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 3\n",
      "============================================================\n",
      "\n",
      "Input (first 200 chars):\n",
      "The appeal raises a short issue of construction under the planning Acts, on which differing views have been expressed by experienced planning judges in the courts below.\n",
      "It arises in the context of a ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Reference Summary:\n",
      "Wolverhampton City Council (the Council), in its capacity as the local planning authority, granted planning permission for four blocks of student accommodation in proximity to a site used for storage of liquefied petroleum gas (LPG).\n",
      "Three of the four blocks of student accommodation had been completed, but work on the fourth had not commenced.\n",
      "Concerned that the LPG storage facility in the vicinit...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Generated Summary:\n",
      "The local planning authority granted permission for the erection of four blocks of student accommodation.\n",
      "The HSE advised that the inner zone of the risk model was 250 metres from the site.\n",
      "The council revoked the permission.\n",
      "The HSE paid compensation to the developer under section 107 of the Town and Country Planning Act 1990.\n",
      "The developer appealed to the Court of Appeal.\n",
      "The Court of Appeal dec...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(min(3, len(predictions))):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXAMPLE {i+1}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nInput (first 200 chars):\\n{test_dataset[i]['judgement'][:200]}...\")\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"\\nReference Summary:\\n{references[i][:400]}...\")\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"\\nGenerated Summary:\\n{predictions[i][:400]}...\")\n",
    "    print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b9449b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:58:31.470227Z",
     "iopub.status.busy": "2026-02-16T06:58:31.469935Z",
     "iopub.status.idle": "2026-02-16T06:58:31.487355Z",
     "shell.execute_reply": "2026-02-16T06:58:31.486643Z",
     "shell.execute_reply.started": "2026-02-16T06:58:31.470194Z"
    },
    "id": "1b9449b9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS COMPARISON\n",
      "============================================================\n",
      "\n",
      "            Model ROUGE-1 ROUGE-2 ROUGE-L Improvement\n",
      " Base (Zero-shot)  0.2213  0.0787  0.1412           -\n",
      "Fine-tuned (LoRA)  0.2872  0.1323  0.1710      +2.98%\n",
      "\n",
      "============================================================\n",
      "Fine-tuning improved ROUGE-L by 2.98%\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create comparison table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Base (Zero-shot)', 'Fine-tuned (LoRA)'],\n",
    "    'ROUGE-1': [\n",
    "        f\"{base_results['rouge1']:.4f}\",\n",
    "        f\"{finetuned_results['rouge1']:.4f}\"\n",
    "    ],\n",
    "    'ROUGE-2': [\n",
    "        f\"{base_results['rouge2']:.4f}\",\n",
    "        f\"{finetuned_results['rouge2']:.4f}\"\n",
    "    ],\n",
    "    'ROUGE-L': [\n",
    "        f\"{base_results['rougeL']:.4f}\",\n",
    "        f\"{finetuned_results['rougeL']:.4f}\"\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        '-',\n",
    "        f\"+{(finetuned_results['rougeL'] - base_results['rougeL']) * 100:.2f}%\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(comparison_df.to_string(index=False))\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(f\"Fine-tuning improved ROUGE-L by {(finetuned_results['rougeL'] - base_results['rougeL']) * 100:.2f}%\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5f586016",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:58:31.488700Z",
     "iopub.status.busy": "2026-02-16T06:58:31.488408Z",
     "iopub.status.idle": "2026-02-16T06:58:31.503447Z",
     "shell.execute_reply": "2026-02-16T06:58:31.502721Z",
     "shell.execute_reply.started": "2026-02-16T06:58:31.488655Z"
    },
    "id": "5f586016",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 1\n",
      "============================================================\n",
      "\n",
      "Judgment (first 300 chars):\n",
      "The appellant brought a claim for judicial review of a decision of the respondent, on 21 February 2012, to approve a Revenue Budget for 2012/13 in relation to the provision of youth services.\n",
      "In his claim form he applied for declarations that the respondent had failed to comply with section 149 of t...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Reference Summary:\n",
      "Mr Aaron Hunt, born on 17 April 1991, suffers from ADHD, learning difficulties and behavioural problems.\n",
      "As a result, North Somerset Council (the Council) are statutorily required, so far as reasonably practicable, to secure access for him to sufficient educational and recreational leisure time activities for the improvement of his well being.\n",
      "On 21 February 2012, the Council made a decision to ap...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Base Model (Zero-shot):\n",
      "This case is about the Court of Appeal\u2019s decision in the case of <strong>R (on the application of Williams) v Secretary of State for Education</strong>, [2013] EWCA Civ 1320.\n",
      "The appellant was concerned about the impact which the reduction in its youth services budget was likely to have on the provision of services for young people with disabilities and in particular on a weekly youth club for vul...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Fine-tuned Model:\n",
      "The respondent approved a revenue budget for 2012/13 in relation to the provision of youth services.\n",
      "The appellant brought a claim for judicial review of the decision on the grounds that the respondent had failed to comply with sections 149 and 507B of the Equality Act 2010 and section 507B of the Education Act 1996.\n",
      "The court found that the respondent had failed in its statutory obligations and t...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 2\n",
      "============================================================\n",
      "\n",
      "Judgment (first 300 chars):\n",
      "Appeal No. 1940 of 1967.\n",
      "Appeal by special leave from the judgment and order dated April 17, 27, 1967 of the Gujarat High Court in Civil Revision Application 328 of 1967.\n",
      "section T. Desai and I. N. Shroff for the appellants.\n",
      "M. P. Amin, P. M. Amin, P. N. Dua and J. B. Dadachanji, for respondent No. ...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Reference Summary:\n",
      "R and F who held a cinema building in Ahmedabad on lease entered on November 27, 1954 into an agreement with respondent No. 1 giving the latter a right to exhibit cinematograph films in the said building.\n",
      "Later respondent No. 1 filed 'suit No. 149 of 1960 to assert his right to exhibit films in the building.\n",
      "The suit resulted in a compromise decree.\n",
      "In pursuance of the compromise a further agreeme...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Base Model (Zero-shot):\n",
      "/27, 1955, be declared valid or invalid and, if invalid, to be set aside and the proceedings of this Court be dismissed with costs.\n",
      "2.\n",
      "Whether the said agreement dated November 27, 1954 as confirmed by their letter dated January 31, 1955, be declared valid or invalid and, if invalid, to be set aside and the proceedings of this Court be dismissed with costs.\n",
      "3.\n",
      "Whether the defendants in the suit be...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Fine-tuned Model:\n",
      "The parties have effectively prevented all progress in the suit pending in the Court of Small Causes at Ahmedabad, the suit was based on the claim by Filmistan as lessees or sub lessees of the theatre and was exclusively triable by the Court of Small Causes by virtue of section 28 of the Bombay Rents, Hotel and Lodging House Rates Control Act, 1947.\n",
      "The suit was amended and additional written stat...\n",
      "\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "EXAMPLE 3\n",
      "============================================================\n",
      "\n",
      "Judgment (first 300 chars):\n",
      "The appeal raises a short issue of construction under the planning Acts, on which differing views have been expressed by experienced planning judges in the courts below.\n",
      "It arises in the context of a planning permission granted by the respondent council for four blocks of student accommodation in pr...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Reference Summary:\n",
      "Wolverhampton City Council (the Council), in its capacity as the local planning authority, granted planning permission for four blocks of student accommodation in proximity to a site used for storage of liquefied petroleum gas (LPG).\n",
      "Three of the four blocks of student accommodation had been completed, but work on the fourth had not commenced.\n",
      "Concerned that the LPG storage facility in the vicinit...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Base Model (Zero-shot):\n",
      "In order to determine whether the HSE's advice is to be given any weight in the planning process, the court must first decide whether the HSE's advice is to be given any weight at all.\n",
      "The question of whether the HSE's advice should be given any weight is a matter of construction.\n",
      "It is a question of law which is to be decided in accordance with the rules of construction (section 118 of the 1990 A...\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Fine-tuned Model:\n",
      "The appellant, Victoria Hall Ltd, sought planning permission to develop four blocks of student accommodation, each comprising 160 rooms and a communal lounge.\n",
      "The planning application was accompanied by a risk assessment and a PADHI+ report.\n",
      "The application was opposed by the respondent, Wolverhampton City Council, on the grounds that it would be contrary to the development plan, that it would be ...\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Display example predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"EXAMPLE {i+1}\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nJudgment (first 300 chars):\\n{test_dataset[i]['judgement'][:300]}...\")\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"\\nReference Summary:\\n{ft_references[i][:400]}...\")\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"\\nBase Model (Zero-shot):\\n{base_predictions[i][:400]}...\")\n",
    "    print(f\"\\n{'-'*60}\")\n",
    "    print(f\"\\nFine-tuned Model:\\n{ft_predictions[i][:400]}...\")\n",
    "    print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b60137",
   "metadata": {
    "id": "a2b60137"
   },
   "source": [
    "---\n",
    "## 9. Inference Function\n",
    "\n",
    "Create a clean, reusable function for generating case summaries in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5bbc55be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:58:31.531599Z",
     "iopub.status.busy": "2026-02-16T06:58:31.531356Z",
     "iopub.status.idle": "2026-02-16T06:58:31.546451Z",
     "shell.execute_reply": "2026-02-16T06:58:31.545825Z",
     "shell.execute_reply.started": "2026-02-16T06:58:31.531568Z"
    },
    "id": "5bbc55be",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Inference function 'summarize_case()' defined\n",
      "\n",
      "Usage:\n",
      "  summary = summarize_case(judgment_text)\n"
     ]
    }
   ],
   "source": [
    "def summarize_case(text, model=model, tokenizer=tokenizer, max_length=256):\n",
    "    \"\"\"\n",
    "    Generate a legal case summary from input judgment text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Legal court judgment text\n",
    "        model: Fine-tuned summarization model\n",
    "        tokenizer: Model tokenizer\n",
    "        max_length (int): Maximum summary length in tokens\n",
    "\n",
    "    Returns:\n",
    "        str: Generated summary\n",
    "    \"\"\"\n",
    "    # Format prompt\n",
    "    prompt = f\"\"\"Instruction:\n",
    "Summarize the following legal court judgment.\n",
    "\n",
    "Input:\n",
    "{text[:3000]}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1280\n",
    "    ).to(model.device)\n",
    "\n",
    "    # Generate with sampling for diversity\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,       # Controls randomness\n",
    "            top_p=0.9,            # Nucleus sampling\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            num_beams=1,          # Greedy sampling for speed\n",
    "        )\n",
    "\n",
    "    # Decode and extract summary\n",
    "    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Extract only the generated response\n",
    "    if \"Response:\" in full_text:\n",
    "        summary = full_text.split(\"Response:\")[-1].strip()\n",
    "    else:\n",
    "        summary = full_text[len(prompt):].strip()\n",
    "\n",
    "    return summary\n",
    "\n",
    "print(\"\u2713 Inference function 'summarize_case()' defined\")\n",
    "print()\n",
    "print(\"Usage:\")\n",
    "print(\"  summary = summarize_case(judgment_text)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d510144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-16T06:58:31.547487Z",
     "iopub.status.busy": "2026-02-16T06:58:31.547278Z",
     "iopub.status.idle": "2026-02-16T06:58:46.261096Z",
     "shell.execute_reply": "2026-02-16T06:58:46.260388Z",
     "shell.execute_reply.started": "2026-02-16T06:58:31.547468Z"
    },
    "id": "4d510144",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing inference function...\n",
      "\n",
      "============================================================\n",
      "Input (first 400 chars):\n",
      "Appeals, Nos. 275 276 of 1963.\n",
      "Appeals by special leave from the judgment and order dated May 2, 1960 of the Kerala High Court in Income tax Referred case No. 98 of 1955(M).\n",
      "section T. Desai, C. V. Mahalingam, B. Parthasarathi and J. B. Dadachanji, for the appellant (in both the appeals).\n",
      "K. N. Rajagopal Sastri and R. N. Sachthey, for the res pondent (in both the appeals).\n",
      "95 December 20, 1963.\n",
      "Th...\n",
      "\n",
      "============================================================\n",
      "\n",
      "Generated Summary:\n",
      "In the year 1946, the appellant, a partnership firm, entered into a partnership agreement with another firm, to transfer the business of the latter to the former.\n",
      "The appellant and the other firm executed a deed of transfer on November 13, 1947.\n",
      "The appellant then made a declaration that it was discontinuing the business of the other firm.\n",
      "The appellant claimed relief under section 25(4) of the Income tax Act, 1922, on the ground that the business was discontinued on February 7, 1948, by the execution of the deed of transfer.\n",
      "The Income tax Officer rejected the claim on the ground that the appellant had not discontinued the business on February 7, 1948, but on November 13, 1947.\n",
      "The Appellate Assistant Commissioner upheld the decision of the Income tax Officer.\n",
      "The appellant, therefore, moved the Income tax Appellate Tribunal which held that the appellant was not entitled to any relief under section 25(4).\n",
      "The Tribunal accordingly directed the Income tax Officer to reopen the assessment.\n",
      "The Income tax Officer reopened the assessment and the appellant was assessed to income tax for the year 19\n",
      "\n",
      "============================================================\n",
      "\n",
      "Reference Summary:\n",
      "By section 25 (4) of the Income tax Act, \"Where the person who was at the commencement of the Indian Income tax (Amendment) Act, 1939.\n",
      "carrying on any business, profession or vocation in which tax was at any time charged under the provisions of the Indian Income tax Act, 1918, is succeeded in such capacity by another person, the change not being merely a change in the constitution of a partnership, no tax shall be payable by the first mentioned person in respect of the income, profits and gains of the period between the end of the previous year and the date of such succession.\" A firm bearing the same name as the appellant firm, had been carrying on business from before 1918 and had paid tax on that business under the Income tax Act, 1918.\n",
      "The firm did three kinds of businesses, namely, (a) in piece goods, yam as general merchants, 92 (b) in the manufacture and sale of umbrellas and (c) in the manufacture and sale of soaps.\n",
      "There were various changes in the constitution of the firm between 1918 and 1934.\n",
      "In May 1939 two, documents were executed, one by the then members of the firm, and a stranger H. being exhibit CI and the other by those members alone,.\n",
      "being exhibit CII.\n",
      "It appeared from exhibit CI that the business in the manufacture and sale of umbrellas and soaps was being carried on from October November 1937 by the parties to it as partners while exhibit CII showed that the parties to it had been carrying on the business in yarn piecegoods and as general merchants as partners from the same time as mentioned in exhibit Cl.\n",
      "On October 30.\n",
      "1943 a document styled as an agreement of partnership was executed by five persons who were then the persons interested in the businesses carried on under the instrument of May 30, 1939.\n",
      "This document referred to the two, agreements of partnership of May 30, 1939 and certain subsequent retirements of partners and admissions of new partners and provided that the businesses previously carried on by the two partnerships.\n",
      "referred to in the instruments of May 30, 1939, would thereafter be carried on by one single partnership constituted by the parties to it.\n",
      "Thereafter all the businesses aforesaid were carried on by this single partnership.\n",
      "The firm constituted by the instrument of October 30, 1943 continued with certain changes in its constitution till February 7, 1948 when the then partners of, it entered into an agreement with a company to transfer the business of the firm to the latter, the transfer to be completed by February 13, 1948 and the transfer was in fact made.\n",
      "The firm constituted by the document of October 30, 1943 claimed relief under section 25(4) in assessment for the years 1948 49 and 1949 50 on the ground that it had been carrying on a business on April 1, 1939 when the Income tax (Amendment) Act, 1939 commenced; to operate on which business tax had been charged under the Act of 1918 and that it was succeeded in that business by a company in February 1948.\n",
      "Held: (per Sarkar and Shah JJ.).\n",
      "The assessee was not entitled to the relief.\n",
      "Cl and CII showed that the business that had been carried on by the firm existing in 1918 was discontinued in October/November 1937 and its businesses were split up into two and from then carried on by two independent partnerships brought into existence by those documents.\n",
      "The old firm was brought to an end by Exs.\n",
      "Cl and CII.\n",
      "When a business carried on in one unit is disintegrated and divided into parts, the parts are not the whole, though all the parts taken together constitute the whole.\n",
      "In such case there is a discontinuance of the original businesses.\n",
      "section N. A. section A. Annamalai, Chettiar vs Commissioner of Income tax, Madras, referred to.\n",
      "93 The business on which tax had been charged under the Act of 1918 was not being carried on April 1, 1939 by the firm which had paid tax under that Act.\n",
      "The business to which the company succeeded under the agreement ,of February 7, 1948 cannot before the succession be said to have been carried on by a firm which was carrying on business on April 1, 1939, for that firm had been newly formed under the instrument of ,October 30, 1943, which expressly revoked the partnership agreements of May 30, 1939 under which two firms had been brought into brought into existence.\n",
      "Per, Hidayatullah J. (dissenting) (i) Sub sections\n",
      "(3) and (4) of section 25 ,of the Act are mutually exclusive . sub section\n",
      "(3) was only applicable when the business was discontinued and that in the term \"succession\" was not to be included a change in the constitution of the partnership.\n",
      "In sub section\n",
      "(4) the emphasis is on succession to a person who on April 1, 1939 was carrying on any business on which tax was at any time ,charged under the Act 1918.\n",
      "In sub section\n",
      "(3) the emphasis is on the discontinuance of the business which had paid tax under the Act 1918.\n",
      "(ii) There is difference of approach to the same facts under the law of partnership and the Income tax law.\n",
      "Charandas vs Haridas, (1960)39 and Dulichand vs ,Commissioner of Income tax, Nagpur, [1956] S.C.R. 154, referred to.\n",
      "(iii) Discontinuance of a firm is not a mere change in the constitution of the firm or even succession where, though the business changes hands, the original business which paid the tax in 1918 is carried on.\n",
      "Shivram Poddar vs Income tax, Officer, C. A. No. 455 of 1963 dated December 13, 1963, referred to.\n",
      "(iv) All cases of discontinuance of businesses are treated under sub section\n",
      "(3) and all cases of succession under sub section\n",
      "(4) and all cases of mere change in the constitution of the firm are neither cases under sub section\n",
      "(3) nor under sub section\n",
      "These sub sections do not apply to cases where the business was not in existence before the Act 1922 came into force.\n",
      "Ambalal Himatlal vs Commissioner of Income tax and Excess Profits Tax, Bombay North, , referred to.\n",
      "(v) Since the soap and umbrella businesses were not in existence and no relief could be claimed in respect of these businesses, changes in respect of them were irrelevant.\n",
      "(vi) by the expression \"discontinued\" in sub section\n",
      "(3) is meant complete cessation of business.\n",
      "In the present case it could be said that this had taken place in respect of the piece goods business; this might 94 have been managed by persons other than those who had paid the tax under the 1918 Act, but the business was not discontinued for the application of sub section\n",
      "Commissioner of Income tax, Bombay vs P. E. Polson, (1945)13 Commissioner of Income tax, West Bengal vs A. W. Figgies and Co. ; and Mevoppar vs Commissioner of Income tax, Madras, 1.\n",
      "L. R. referred to.\n",
      "(vii) In the present case there was no succession and it falls within the rule laid down by this Court in Figgies ' case.\n",
      "(viii) Though a firm was to be regarded as an entity for the purpose of the Income tax Act, that entity was not to be taken to be disturbed by the coming in or going out of partners.\n",
      "Applying the test to the present case it was held that the identity of the entity was never lost and there was never a succession till the year 1948.\n",
      "No question of the dissolution of the old firm in piece goods business ever arose.\n",
      "It continued right through, even other newly started businesses were owned by it.\n",
      "It cannot be said that the old firm had either discontinued or had been succeeded by another person.\n",
      "Hemchand was a mere employee though described as a partner.\n",
      "The entry of Hemehand did not constitute a dissolution of the old firm.\n",
      "Commissioner of Income tax, Bombay City vs Kolhia Hirdagarh, Co. Ltd., Bombay, (1949) 17 ; and Commissioner of Income tax, Bombay City vs Sir Homi Metters Executor, , referred to.\n",
      "(ix) The appellants are entitled to succeed in their claim regarding the business in piece goods yarn and banking which alone had paid tax under the 1918 Act.\n",
      "\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Test inference function\n",
    "print(\"Testing inference function...\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_case = test_dataset[5]['judgement']\n",
    "test_summary = summarize_case(test_case)\n",
    "\n",
    "print(f\"Input (first 400 chars):\\n{test_case[:400]}...\\n\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nGenerated Summary:\\n{test_summary}\\n\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nReference Summary:\\n{test_dataset[5]['summary']}\\n\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82525329",
   "metadata": {
    "id": "82525329"
   },
   "source": [
    "---\n",
    "## 11. Save and Reload Model\n",
    "\n",
    "Demonstrate how to save the LoRA adapters and reload for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9124d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-16T06:58:46.281454Z",
     "iopub.status.idle": "2026-02-16T06:58:46.281831Z",
     "shell.execute_reply": "2026-02-16T06:58:46.281641Z",
     "shell.execute_reply.started": "2026-02-16T06:58:46.281622Z"
    },
    "id": "3cb9124d",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save LoRA adapters (already done during training)\n",
    "print(\"LoRA adapters have been saved to:\", OUTPUT_DIR)\n",
    "print()\n",
    "print(\"Files saved:\")\n",
    "for file in os.listdir(OUTPUT_DIR):\n",
    "    if not file.startswith('checkpoint'):\n",
    "        file_size = os.path.getsize(os.path.join(OUTPUT_DIR, file)) / (1024 * 1024)\n",
    "        print(f\"  - {file} ({file_size:.2f} MB)\")\n",
    "\n",
    "print()\n",
    "print(\"Total LoRA adapter size: ~10-50 MB (vs ~5GB for full model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923040a-bbcb-4a89-876f-8f77966754bd",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31259,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}